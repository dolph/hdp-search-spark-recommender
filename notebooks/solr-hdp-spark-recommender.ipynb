{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating a Scalable Recommender with Apache Spark & Apache Solr\n",
    "\n",
    "In this notebook, you will create a recommendation engine using Spark and Solr. Using some movie rating data,\n",
    "you will train a collaborative filtering model in Spark and export the trained model to Solr. Once exported, \n",
    "you can test your recommendations by querying Solr and displaying the results.\n",
    "\n",
    "### _Prerequisites_\n",
    "\n",
    "The notebook assumes you have installed Solr, the Solr vector-scoring plugin, Apache Spark and the Solr Spark connector detailed in the [setup steps](https://github.com/IBM/hdp-search-spark-recommender#steps).\n",
    "\n",
    "> _Optional:_\n",
    "\n",
    "> In order to display the images in the recommendation demo, you will need to access [The Movie Database (TMdb) API](https://www.themoviedb.org/documentation/api). Please follow the [instructions](https://developers.themoviedb.org/3/getting-started) to get an API key.\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will work through the following steps\n",
    "\n",
    "1. Prepare the data\n",
    "2. Use the Solr Spark connector to save it to Solr\n",
    "3. Load ratings data and train a collaborative filtering recommendation model using Spark MLlib\n",
    "3. Save the model to Solr\n",
    "4. Show recommendations using Solr vector scoring plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Prepare the data\n",
    "\n",
    "* This notebook uses the \"small\" version of the latest MovieLens movie rating dataset, containing about 100,000 ratings, 9,000 movies and 700 users\n",
    "* The latest version of the data can be downloaded at https://grouplens.org/datasets/movielens/latest/\n",
    "* Download the `ml-latest-small.zip` file and unzip it to a suitable location on your system.\n",
    "\n",
    "The folder should contain a number of CSV files. We will be using the following files:\n",
    "* `ratings.csv` - movie rating data\n",
    "* `links.csv` - external database ids for each movie\n",
    "* `movies.csv` - movie title and genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tmdbsimple --user\n",
    "!pip install solrcloudpy --user\n",
    "!pip install simplejson --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# delete any existing Spark session\n",
    "%spark delete -s spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "livyURL=\"PROVIDE_LIVY_URL\"\n",
    "%spark add -s spark -l python -u $livyURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# first import a few utility methods that we'll use later on\n",
    "from IPython.display import Image, HTML, display\n",
    "from urllib2 import *\n",
    "import simplejson as json\n",
    "import requests as requests\n",
    "import sys\n",
    "from solrcloudpy.connection import SolrConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# first import a few utility methods that we'll use later on\n",
    "from IPython.display import Image, HTML, display\n",
    "from urllib2 import *\n",
    "import simplejson as json\n",
    "import requests as requests\n",
    "import sys\n",
    "import paramiko\n",
    "from solrcloudpy.connection import SolrConnection\n",
    "# check PySpark is running\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load rating and movie data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Ratings**\n",
    "\n",
    "The ratings data consists of around 100,000 ratings given by users to movies. Each row of the `DataFrame` consists of a `userId`, `movieId` and `timestamp` for the event, together with the `rating` given by the user to the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# point to the HDFS location where you unzipped the movie ratings data\n",
    "HDFS_URL_FOR_DATA = \"PROVIDE_PATH_TO_YOUR_DATA\"\n",
    "# load ratings data\n",
    "ratings = spark.read.csv(HDFS_URL_FOR_DATA + \"/ratings.csv\", header=True, inferSchema=True)\n",
    "ratings.cache()\n",
    "print(\"Number of ratings: %i\" % ratings.count())\n",
    "print(\"Sample of ratings:\")\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You will see that the `timestamp` field is a UNIX timestamp in seconds. In the following cell, a spark built-in function `from_unix_timestap` is called to convert the timestamp to the format Solr acccepts. More information on Solr date formats can be found in https://lucene.apache.org/solr/guide/6_6/working-with-dates.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql import functions as F\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\", F.from_unixtime(\"timestamp\").alias(\"timestamp\"))\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Movies**\n",
    "\n",
    "The file `movies.csv` contains the `movieId`, `title` and `genres` for each movie. As you can see, the `genres` field is a bit tricky to use, as the genres are in the form of one string delimited by the `|` character: `Adventure|Animation|Children|Comedy|Fantasy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# load raw data from CSV\n",
    "raw_movies = spark.read.csv(HDFS_URL_FOR_DATA + \"/movies.csv\", header=True, inferSchema=True)\n",
    "print(\"Raw movie data:\")\n",
    "raw_movies.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a `DataFrame` user-defined function (UDF) to extract this delimited string into a list of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "# define a UDF to convert the raw genres string to an array of genres and lowercase\n",
    "extract_genres = udf(lambda x: x.lower().split(\"|\"), ArrayType(StringType()))\n",
    "# test it out\n",
    "raw_movies.select(\"movieId\", \"title\", extract_genres(\"genres\").alias(\"genres\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ok, that looks better!\n",
    "\n",
    "You may also notice that the movie titles contain the year of release. It would be useful to have that as a field in your search index for filtering results (say you want to filter our recommendations to include only more recent movies).\n",
    "\n",
    "Create a UDF to extract the release year from the title using a Python regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import re\n",
    "# define a UDF to extract the release year from the title, and return the new title and year in a struct type\n",
    "def extract_year_fn(title):\n",
    "    result = re.search(\"\\(\\d{4}\\)\", title)\n",
    "    try:\n",
    "        if result:\n",
    "            group = result.group()\n",
    "            year = group[1:-1]\n",
    "            start_pos = result.start()\n",
    "            title = title[:start_pos-1]\n",
    "            return (title, year)\n",
    "        else:\n",
    "            return (title, 1970)\n",
    "    except:\n",
    "        print(title)\n",
    "\n",
    "extract_year = udf(extract_year_fn,\\\n",
    "                   StructType([StructField(\"title\", StringType(), True),\\\n",
    "                               StructField(\"release_date\", StringType(), True)]))\n",
    "    \n",
    "# test out our function\n",
    "s = \"Jumanji (1995)\"\n",
    "extract_year_fn(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ok the function works! Now create a new `DataFrame` with the cleaned-up titles, release dates and genres of the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "movies = raw_movies.select(\n",
    "    \"movieId\", extract_year(\"title\").title.alias(\"title\"),\\\n",
    "    extract_year(\"title\").release_date.alias(\"release_date\"),\\\n",
    "    extract_genres(\"genres\").alias(\"genres\"))\n",
    "print(\"Cleaned movie data:\")\n",
    "movies.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, join the `links.csv` data to `movies` so that there is an id for _The Movie Database_ corresponding to each movie. You can use this id to retrieve movie poster images when displaying your recommendations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "link_data = spark.read.csv(HDFS_URL_FOR_DATA + \"/links.csv\", header=True, inferSchema=True)\n",
    "# join movies with links to get TMDB id\n",
    "movie_data = movies.join(link_data, movies.movieId == link_data.movieId)\\\n",
    "    .select(movies.movieId, movies.title, movies.release_date, movies.genres, link_data.tmdbId)\n",
    "num_movies = movie_data.count()\n",
    "print(\"Cleaned movie data with tmdbId links:\")\n",
    "movie_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **_Optional_**\n",
    "\n",
    "> Run the below cell to test your access to TMDb API. You should see the _Toy Story_ movie poster displayed inline.\n",
    "\n",
    "> To install the Python package run `pip install tmdbsimple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "try:\n",
    "    import tmdbsimple as tmdb\n",
    "    # replace this variable with your actual TMdb API key\n",
    "    tmdb.API_KEY='PROVIDE_YOUR_TMDB_API_KEY'\n",
    "    print(\"Successfully imported tmdbsimple!\")\n",
    "    # base URL for TMDB poster images\n",
    "    IMAGE_URL = 'https://image.tmdb.org/t/p/w500'\n",
    "    movie_id = movie_data.first().tmdbId\n",
    "    movie_info = tmdb.Movies(movie_id).info()\n",
    "    movie_poster_url = IMAGE_URL + movie_info['poster_path']\n",
    "    # display(Image(movie_poster_url, width=200))\n",
    "    selectQuery = \"SELECT '%s'\" % (movie_poster_url)\n",
    "    poster_df = spark.sql(selectQuery)\n",
    "    poster_df.registerTempTable('poster')\n",
    "except Exception:\n",
    "    print(\"Cannot import tmdbsimple, no movie posters will be displayed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -c sql -o poster_url -q\n",
    "select * from poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Image(poster_url.iloc[0][0], width=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setting of global variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*** Fill in the following variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Host name where Solr is installed\n",
    "SOLR_HOST = 'PROVIDE_SOLR_HOST_NAME'\n",
    "# The port Solr server is listening at. If you have configured Solr to listen on a\n",
    "# different port than the default then please change the value accordingly.\n",
    "SOLR_PORT = '8983'\n",
    "SOLR_HOST_PORT = SOLR_HOST + ':' + SOLR_PORT\n",
    "SOLR_URL = 'http://' + SOLR_HOST_PORT + '/solr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Install directory needs to be specified to aid in moving Solr config files to\n",
    "# zookeeper. This is a requirement in Solr cloud setup. Typically Solr is installed\n",
    "# at '/opt/lucidworks-hdpsearch'\n",
    "SOLR_INSTALL_DIR = 'PROVIDE_SOLR_INSTALL_DIR'\n",
    "# Host name where Solr is installed\n",
    "SOLR_HOST = 'PROVIDE_SOLR_HOST_NAME'\n",
    "# The port Solr server is listening at. If you have configured Solr to listen on a\n",
    "# different port than the default then please change the value accordingly.\n",
    "SOLR_PORT = '8983'\n",
    "# Host name where zookeeper is installed.\n",
    "ZKHOST = 'PROVIDE_ZOOKEPER_HOST_NAME'\n",
    "# Zookeeper port. If you have configured zookeeper to listen on a\n",
    "# different port than the default then please change the value accordingly.\n",
    "ZKPORT = '2181'\n",
    "# Required in order to ssh to the machine where the Solr server is installed in order\n",
    "# to copy the Solr configuration files into zookeeper. If you don't want to\n",
    "# do this step from the notebook, then you can do this outside the notebook and\n",
    "# do the remaining steps like creating collection and setting up schemas from notebook.\n",
    "SSHUSER = 'PROVIDE_SSH_USER'\n",
    "SSHPASSWORD = 'PROVIDE_SSH_PASSWORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Derived variables used in the following cells\n",
    "SOLR_HOST_PORT = SOLR_HOST + ':' + SOLR_PORT\n",
    "SOLR_URL = 'http://' + SOLR_HOST_PORT + '/solr'\n",
    "SOLR_ADMIN_URL = SOLR_URL + '/admin/'\n",
    "SCHEMA_URL = SOLR_URL + '/solr/admin/'\n",
    "ZK_URL = ZKHOST + ':' + ZKPORT + '/solr'\n",
    "json_response = '&wt=json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Utility functions that helps in performing admin functions against Solr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "def executeSshCommand(command):\n",
    "  ssh = None\n",
    "  try:\n",
    "      ssh = paramiko.SSHClient()\n",
    "      ssh.load_system_host_keys()\n",
    "      ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "      ssh.connect(SOLR_HOST, username=SSHUSER, password=SSHPASSWORD)\n",
    "      ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(command)\n",
    "  finally:\n",
    "      if ssh:\n",
    "         ssh.close()\n",
    "\n",
    "def createJKConfig(collectionName):\n",
    "  cmd = (SOLR_INSTALL_DIR + '/solr/server/scripts/cloud-scripts' +\n",
    "        '/zkcli.sh -zkhost ' + ZK_URL +\n",
    "        ' -cmd upconfig -confdir ' +\n",
    "        SOLR_INSTALL_DIR + '/' + 'solr/server/solr/configsets/data_driven_schema_configs/conf/ -confname ' + collectionName\n",
    "        )\n",
    "  executeSshCommand(cmd)\n",
    "    \n",
    "def deleteJKConfig(collectionName):\n",
    "  dirToClear = 'configs/collectionName'\n",
    "  cmd = (SOLR_INSTALL_DIR + '/solr/server/scripts/cloud-scripts' +\n",
    "        '/zkcli.sh -zkhost ' + ZK_URL +\n",
    "        ' -cmd clear ' + dirToClear\n",
    "        )\n",
    "  executeSshCommand(cmd)\n",
    "\n",
    "def displayStatus():\n",
    "  request = SOLR_ADMIN_URL + 'info/system?'+ json_response\n",
    "  try:\n",
    "    connection = urlopen(request)\n",
    "    response = json.load(connection)\n",
    "    responseCode = response[\"responseHeader\"][\"status\"]\n",
    "    if responseCode == 0:\n",
    "      print(response)\n",
    "    else:\n",
    "      print(\"Error: \", responseCode)\n",
    "  except URLError, e:\n",
    "    print e.reason\n",
    "\n",
    "def createCollection(collectionName):\n",
    "  createJKConfig(collectionName)\n",
    "  action_string = 'collections?action=CREATE&name=' + \\\n",
    "                  collectionName + \\\n",
    "                  '&numShards=1&replicationFactor=1'\n",
    "  request = SOLR_ADMIN_URL + action_string + json_response\n",
    "  try:\n",
    "    connection = urlopen(request)\n",
    "    response = json.load(connection)\n",
    "    responseCode = response[\"responseHeader\"][\"status\"]\n",
    "    if responseCode == 0:\n",
    "      print(\"Successfully created collection: \", collectionName)\n",
    "    else:\n",
    "      print(\"Error: \", responseCode)\n",
    "  except URLError, e:\n",
    "    pass \n",
    "\n",
    "def dropCollection(collectionName):\n",
    "  action_string = 'collections?action=DELETE&name=' + collectionName\n",
    "  request = SOLR_ADMIN_URL + action_string + json_response\n",
    "  try:\n",
    "    connection = urlopen(request)\n",
    "    response = json.load(connection)\n",
    "    responseCode = response[\"responseHeader\"][\"status\"]\n",
    "    if responseCode == 0:\n",
    "      print(\"Successfully removed collection: \", collectionName)\n",
    "    else:\n",
    "      print(\"Error: \", responseCode)\n",
    "  except URLError, e:\n",
    "    print e.reason\n",
    "  deleteJKConfig(collectionName)\n",
    "\n",
    "def setupSchema():\n",
    "  try:\n",
    "    dropCollection('ratings')\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    dropCollection('users')\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    dropCollection('movies')\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    dropCollection('movies_vector')\n",
    "  except:\n",
    "    pass\n",
    "  createCollection('ratings')\n",
    "  createCollection('users')\n",
    "  createCollection('movies')\n",
    "  createCollection('movies_vector')\n",
    "  print(\"Created collections : [ratings, users, movies, movies_vector]\")\n",
    "  ratings_schema = \"\"\"{\n",
    "    \"add-field\":{\n",
    "       \"name\":\"timestamp\",\n",
    "       \"type\":\"date\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"userId\",\n",
    "       \"type\":\"int\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"movieId\",\n",
    "       \"type\":\"int\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"rating\",\n",
    "       \"type\":\"double\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     }\n",
    "  }\"\"\"\n",
    "  users_schema = \"\"\"{\n",
    "    \"add-field\":{\n",
    "       \"name\":\"id\",\n",
    "       \"type\":\"int\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"factor\",\n",
    "       \"type\":\"VectorField\",\n",
    "       \"termOffsets\":\"true\",\n",
    "       \"termPositions\":\"true\",\n",
    "       \"termVectors\":\"true\",\n",
    "       \"multiValued\":\"true\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"version\",\n",
    "       \"type\":\"string\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\",\n",
    "       \"multiValued\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"timestamp\",\n",
    "       \"type\":\"date\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     }\n",
    "  }\"\"\"\n",
    "  movies_schema = \"\"\"{\n",
    "    \"add-field\":{\n",
    "       \"name\":\"movieId\",\n",
    "       \"type\":\"int\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"tmdbId\",\n",
    "       \"type\":\"string\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\",\n",
    "       \"multiValued\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"title\",\n",
    "       \"type\":\"text_general\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"genres\",\n",
    "       \"type\":\"string\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\",\n",
    "       \"multiValued\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"release_date\",\n",
    "       \"type\":\"int\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"factor\",\n",
    "       \"type\":\"VectorField\",\n",
    "       \"termOffsets\":\"true\",\n",
    "       \"termPositions\":\"true\",\n",
    "       \"termVectors\":\"true\",\n",
    "       \"multiValued\":\"true\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"version\",\n",
    "       \"type\":\"string\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\",\n",
    "       \"multiValued\":\"true\"\n",
    "     },\n",
    "    \"add-field\":{\n",
    "       \"name\":\"timestamp\",\n",
    "       \"type\":\"date\",\n",
    "       \"stored\":\"true\",\n",
    "       \"indexed\":\"true\"\n",
    "    }\n",
    "  }\"\"\"\n",
    "  mappings_schema_url = SOLR_URL + '/ratings/schema'\n",
    "  headers = {'content-type': 'application/json'}\n",
    "  ratings_status = requests.post(mappings_schema_url, data=ratings_schema, headers=headers)\n",
    "  if ratings_status.status_code == requests.codes.ok:\n",
    "     print(\"Successfully registered schema for collection ratings\")\n",
    "  else:\n",
    "     ratings_status.raise_for_status()\n",
    "  users_schema_url = SOLR_URL + '/users/schema'\n",
    "  users_status  = requests.post(users_schema_url, data=users_schema, headers=headers)\n",
    "  if users_status.status_code == requests.codes.ok:\n",
    "     print(\"Successfully registered schema for collection users\")\n",
    "  else:\n",
    "     users_status.raise_for_status()\n",
    "  movies_schema_url = SOLR_URL + '/movies/schema'\n",
    "  movies_status = requests.post(movies_schema_url, data=movies_schema, headers=headers)\n",
    "  if movies_status.status_code == requests.codes.ok:\n",
    "     print(\"Successfully registered schema for collection movies\")\n",
    "  else:\n",
    "     movies_status.raise_for_status()\n",
    "  movies_vector_schema_url = SOLR_URL + '/movies_vector/schema'\n",
    "  movies_vector_status = requests.post(movies_vector_schema_url, data=movies_schema, headers=headers)\n",
    "  if movies_vector_status.status_code == requests.codes.ok:\n",
    "     print(\"Successfully registered schema for collection movies_vector\")\n",
    "  else:\n",
    "     movies_vector_status.raise_for_status()     \n",
    "\n",
    "def commit(conn, collection):\n",
    "    conn[collection].commit()\n",
    "\n",
    "def prettyPrint(docs):\n",
    "    for x in docs:\n",
    "       print(json.dumps(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Load data into Solr\n",
    "\n",
    "Now that you have your dataset processed and prepared, you will load it into Solr.\n",
    "\n",
    "_Note:_ for the purposes of this demo notebook you have started with an existing example dataset and will load that into Solr. In practice you may write your event data as well as user and item metadata from your application directly into Solr.\n",
    "\n",
    "First test that your Solr instance is running and you can connect to it using the Solr Admin API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "displayStatus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Solr collections with schema mappings for users, movies and rating events\n",
    "\n",
    "Creates the Solr collections and explicitly authors their schema. While Solr supports dynamic mapping, it's advisable to specify the schema mapping explicitly if you know what your data looks like.\n",
    "\n",
    "For the purposes of your recommendation engine, this is also necessary so that you can specify a custom analyzer for the field that will hold the recommendation \"model\" (that is, the factor vectors). This will ensure the vector-scoring plugin will work correctly.\n",
    "\n",
    "> _Note:_ This notebook does not go into detail about the underlying scoring mechanism or the relevant Solr internals. See the talks and slides in the [Journey Links section](https://github.com/MLnick/elasticsearch-spark-recommender-demo/blob/master/README.md#links) for more detail.\n",
    "\n",
    "__References:__\n",
    "* [Create index request](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html)\n",
    "* [Delimited payload filter](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/analysis-delimited-payload-tokenfilter.html)\n",
    "* [Term vectors](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/docs-termvectors.html#_term_information)\n",
    "* [Mapping](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/mapping.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **Note**\n",
    "\n",
    "> Please note that the following cell drops the collections first and then recreates them with appropriate schema. Please make sure you don't have existing collections with the same name. You may inadvertently delete the collections with the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you're ready to create the collections and author their respective schema definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "setupSchema()\n",
    "conn = SolrConnection(SOLR_HOST_PORT, version=\"6.6.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = SolrConnection(SOLR_HOST_PORT, version=\"6.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load Ratings and Movies DataFrames into Solr\n",
    "\n",
    "First you will write the ratings data to Solr. Notice that you can simply use the Spark Solr connector to write a `DataFrame` with the native Spark datasource API by specifying `format(\"solr\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# write ratings data\n",
    "ratings.write.format(\"solr\").option(\"zkhost\", ZK_URL).option(\"collection\", \"ratings\").save(\"ratings\")\n",
    "commit(conn, \"ratings\")\n",
    "# check write went ok\n",
    "print(\"Dataframe count: %d\" % ratings.count())\n",
    "res = conn[\"ratings\"].search({'q':'*:*'})\n",
    "print(\"Solr index count:  %d\" % res.result.response.numFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# test things out by retrieving a few rating event documents from Solr\n",
    "from solrcloudpy.parameters import SearchOptions\n",
    "se = SearchOptions()\n",
    "se.commonparams.q(\"*\").rows(3)\n",
    "res = conn[\"ratings\"].search(se)\n",
    "prettyPrint(res.result.response.docs)\n",
    "print(\"Number of documents returned from Query %d\" % len(res.result.response.docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since you've indexed the rating event data into Solr, you can use all the capabilities of a search engine to query the data. For example, you could count the number of ratings events in a given date range using Solr's date math in a query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "se = SearchOptions()\n",
    "se.commonparams.q(\"timestamp:[2016-01-01T00:00:00Z  TO 2016-02-01T23:59:59Z]\")\n",
    "res = conn[\"ratings\"].search(se)\n",
    "prettyPrint(res.result.response.docs)\n",
    "print(\"Number of documents returned from Query %d\" % len(res.result.response.docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next write the movie metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# write movie data\n",
    "movie_data.write.format(\"solr\").option(\"zkhost\", ZK_URL).option(\"collection\", \"movies\").save(\"movies\")\n",
    "commit(conn, \"movies\")\n",
    "# check load went ok\n",
    "print(\"Movie DF count: %d\" % movie_data.count())\n",
    "res = conn[\"movies\"].search({'q':'*:*'})\n",
    "print(\"Solr index count:  %d\" % res.result.response.numFound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Again you can harness the power of search to query the movie metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# test things out by searching for movies containing \"matrix\" in the title\n",
    "se = SearchOptions()\n",
    "se.commonparams.q(\"title:matrix\").rows(3)\n",
    "res = conn[\"movies\"].search(se)\n",
    "prettyPrint(res.result.response.docs)\n",
    "print(\"Number of documents returned from Query %d\" % len(res.result.response.docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Train a recommmender model on the ratings data\n",
    "\n",
    "Your data is now stored in Solr and you will use the ratings data to build a collaborative filtering recommendation model.\n",
    "\n",
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) is a recommendation approach that is effectively based on the \"wisdom of the crowd\". It makes the assumption that if two people share similar preferences, then the things that one of them prefers could be good recommendations to make to the other. In other words, if user A tends to like certain movies, and user B shares some of these preferences with user A, then the movies that user A likes, that user B _has not yet seen_, may well be movies that user B will also like.\n",
    "\n",
    "In a similar manner, we can think about _items_ as being similar if they tend to be rated highly by the same people, on average. \n",
    "\n",
    "Hence these models are based on the combined, collaborative preferences and behavior of all users in aggregate. They tend to be very effective in practice (provided you have enough preference data to train the model). The ratings data you have is a form of _explicit preference data_, perfect for training collaborative filtering models.\n",
    "\n",
    "### Alternating Least Squares\n",
    "\n",
    "Alternating Least Squares (ALS) is a specific algorithm for solving a type of collaborative filtering model known as [matrix factorization (MF)](https://en.wikipedia.org/wiki/Matrix_decomposition). The core idea of MF is to represent the ratings as a _user-item ratings matrix_. In the diagram below you will see this matrix on the left (with users as _rows_ and movies as _columns_). The entries in this matrix are the ratings given by users to movies.\n",
    "\n",
    "You may also notice that the matrix has _missing entries_ because not all users have rated all movies. In this situation we refer to the data as _sparse_.\n",
    "\n",
    "![als-diagram.png](../doc/source/images/als-diagram.png)\n",
    "\n",
    "MF methods aim to find two much smaller matrices (one representing the _users_ and the other the _items_) that, when multiplied together, re-construct the original ratings matrix as closely as possible. This is know as _factorizing_ the original matrix, hence the name of the technique.\n",
    "\n",
    "The two smaller matrices are called _factor matrices_ (or _latent features_). The user and movie factor matrices are illustrated on the right in the diagram above. The idea is that each user factor vector is a compressed representation of the user's preferences and behavior. Likewise, each item factor vector is a compressed representation of the item. Once the model is trained, the factor vectors can be used to make recommendations, which is what you will do in the following sections.\n",
    "\n",
    "__Further reading:__\n",
    "\n",
    "* [Spark MLlib Collaborative Filtering](http://spark.apache.org/docs/latest/ml-collaborative-filtering.html)\n",
    "* [Alternating Least Squares and collaborative filtering](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/)\n",
    "* [Quora question on Alternating Least Squares](https://www.quora.com/What-is-the-Alternating-Least-Squares-method-in-recommendation-systems-And-why-does-this-algorithm-work-intuition-behind-this)\n",
    "\n",
    "Fortunately, Spark's MLlib machine learning library has a scalable, efficient implementation of matrix factorization built in, which we can use to train our recommendation model. Next, you will use Spark's ALS to train a model on your ratings data from Solr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "ratings_from_solr = spark.read.format(\"solr\").option(\"zkhost\", ZK_URL).option(\"collection\", \"ratings\").load(\"ratings\")\n",
    "ratings_from_solr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", regParam=0.01, rank=20, seed=12)\n",
    "model = als.fit(ratings_from_solr)\n",
    "model.userFactors.show(5)\n",
    "model.itemFactors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Export ALS user and item factor vectors to Solr\n",
    "\n",
    "Congratulations, you've trained a recommendation model! The next step is to export the model factors (shown in the `DataFrames` above) to Solr.\n",
    "\n",
    "In order to store the model in the correct format for the index mappings set up earlier, you will need to create some utility functions. These functions will allow you to convert the raw vectors (which are equivalent to a Python list in the factor `DataFrames` above) to the correct _delimited string format_. This ensures Solr will parse the vector field in the model correctly using the delimited token filter custom analyzer you configured earlier.\n",
    "\n",
    "You will also create a function to convert a vector and related metadata (such as the Spark model id and a timestamp) into a `DataFrame` field that matches the `model` field in the Solr index mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Utility functions for converting factor vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, lit, current_timestamp, unix_timestamp\n",
    "\n",
    "def convert_vector(x):\n",
    "    '''Convert a list or numpy array to delimited token filter format'''\n",
    "    return \" \".join([\"%s|%s\" % (i, v) for i, v in enumerate(x)])\n",
    "\n",
    "def reverse_convert(s):\n",
    "    '''Convert a delimited token filter format string back to list format'''\n",
    "    return  [float(f.split(\"|\")[1]) for f in s.split(\" \")]\n",
    "\n",
    "def vector_to_struct(x, version, ts):\n",
    "    '''Convert a vector to a SparkSQL Struct with string-format vector and version fields'''\n",
    "    return (convert_vector(x), version, ts)\n",
    "\n",
    "vector_struct = udf(vector_to_struct, \\\n",
    "                    StructType([StructField(\"factor\", StringType(), True), \\\n",
    "                                StructField(\"version\", StringType(), True),\\\n",
    "                                StructField(\"timestamp\", LongType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# test out the vector conversion function\n",
    "test_vec = model.userFactors.select(\"features\").first().features\n",
    "print(test_vec)\n",
    "print()\n",
    "print(convert_vector(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert factor vectors to [factor, version, timestamp] form and write to Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "ver = model.uid\n",
    "ts = unix_timestamp(current_timestamp())\n",
    "movie_vectors = model.itemFactors.select(\"id\", vector_struct(\"features\", lit(ver), ts).alias(\"@model\"))\n",
    "movie_vectors.select(\"id\", \"@model.factor\", \"@model.version\", \"@model.timestamp\").show(5)\n",
    "user_vectors = model.userFactors.select(\"id\", vector_struct(\"features\", lit(ver), ts).alias(\"@model\"))\n",
    "user_vectors.select(\"id\", \"@model.factor\", \"@model.version\", \"@model.timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "movie_vectors.registerTempTable(\"movie_vector\")\n",
    "movie_data.registerTempTable(\"movie_data\")\n",
    "\n",
    "# Inner join to fill in the extra info from movie vector\n",
    "movie_vector_data = spark.sql(\"select * from movie_data, movie_vector where movieId = id\").drop(\"id\")\n",
    "movie_vector_data.registerTempTable(\"movie_vector_data\")\n",
    "movie_vector_data = \\\n",
    "    movie_vector_data.select( \\\n",
    "      \"movieId\", \"tmdbId\", \"title\", \"release_date\", \\\n",
    "      \"genres\", \"@model.factor\", \"@model.version\", \\\n",
    "       F.from_unixtime(\"@model.timestamp\").alias(\"timestamp\"))\n",
    "# Now write to Solr\n",
    "movie_vector_data.write.format(\"solr\").option(\"zkhost\", ZK_URL).option(\"collection\", \"movies_vector\").save(\"movies_vector\")\n",
    "commit(conn, \"movies_vector\")\n",
    "# check load went ok\n",
    "print(\"Movie DF count: %d\" % movie_vector_data.count())\n",
    "res = conn[\"movies_vector\"].search({'q':'*:*'})\n",
    "print(\"Solr index count:  %d\" % res.result.response.numFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# write data to Solr.\n",
    "users_vector = \\\n",
    "    user_vectors.select(\"id\", \"@model.factor\", \"@model.version\", \\\n",
    "                         F.from_unixtime(\"@model.timestamp\").alias(\"timestamp\"))\n",
    "users_vector.write.format(\"solr\").option(\"zkhost\", ZK_URL).option(\"collection\", \"users\").save(\"users\")\n",
    "commit(conn, \"users\")\n",
    "print(\"Users DF count: %d\" % users_vector.count())\n",
    "res = conn[\"users\"].search({'q':'*:*'})\n",
    "print(\"Solr index count:  %d\" % res.result.response.numFound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check the data  was written correctly\n",
    "\n",
    "You can search for a movie to see if the model factor vector was written correctly. You should see a `'@model': {'factor': '0|...` field in the returned movie document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "# search for a particular sci-fi movie\n",
    "se = SearchOptions()\n",
    "se.commonparams.q(\"star wars phantom menace\").rows(1)\n",
    "res = conn[\"movies_vector\"].search(se)\n",
    "print(\"Solr index count:  %d\" % res.result.response.numFound)\n",
    "prettyPrint(res.result.response.docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 5: Recommend using Solr!\n",
    "\n",
    "Now that you have loaded your recommendation model into Solr, you will generate some recommendations.\n",
    "First, you will need to create a few utility functions for:\n",
    "\n",
    "* Fetching movie posters from TMdb API (optional)\n",
    "* Constructing the Solr query to generate recommendations from your factor model\n",
    "* Given a movie, use this query to find the movies most similar to it\n",
    "* Given a user, use this query to find the movies with the highest predicted rating, to recommend to the user\n",
    "* Display the results as an HTML table in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "from solrcloudpy.parameters import SearchOptions\n",
    "\n",
    "def reverse_convert(s):\n",
    "    '''Convert a delimited token filter format string back to list format'''\n",
    "    return  [float(f.split(\"|\")[1]) for f in s.split(\" \")]\n",
    "\n",
    "def get_poster_url(id):\n",
    "    \"\"\"Fetch movie poster image URL from TMDb API given a tmdbId\"\"\"\n",
    "    IMAGE_URL = 'https://image.tmdb.org/t/p/w500'\n",
    "    \n",
    "    try:\n",
    "        import tmdbsimple as tmdb\n",
    "        tmdb.API_KEY='YOUR_TMDB_API_KEY'\n",
    "        from tmdbsimple import APIKeyError\n",
    "        try:\n",
    "            movie = tmdb.Movies(id).info()\n",
    "            poster_url = IMAGE_URL + movie['poster_path'] if 'poster_path' in movie and movie['poster_path'] is not None else \"\"\n",
    "            return poster_url\n",
    "        except APIKeyError as ae:\n",
    "            return \"KEY_ERR\"\n",
    "    except Exception as me:\n",
    "        print(str(me))\n",
    "        return \"NA\"\n",
    "    \n",
    "    \n",
    "def fn_query(query_vec, q=\"\", cosine=False):\n",
    "    \"\"\"\n",
    "    Construct a Solr query that produces score for each document based on training model data.\n",
    "    \n",
    "    The query takes as parameters:\n",
    "        - the field in the candidate document that contains the factor vector\n",
    "        - the query vector\n",
    "        - a flag indicating whether to use dot product or cosine similarity (normalized dot product) for scores\n",
    "        \n",
    "    The query vector passed in will be the user factor vector (if generating recommended movies for a user)\n",
    "    or movie factor vector (if generating similar movies for a given movie)\n",
    "    \"\"\"\n",
    "    query_vec_str = \",\".join(map(str, query_vec))\n",
    "    query = '{!vp f=factor vector = \"' + query_vec_str + '\"' + ' cosine = ' + str(cosine).lower() + '}'\n",
    "    return query\n",
    "\n",
    "def get_similar(the_id, q=\"\", num=10, index=\"demo\", dt=\"movies_vector\"):\n",
    "    \"\"\"\n",
    "    Given a movie id, execute the recommendation function score query to find similar movies, ranked by cosine similarity\n",
    "    \"\"\"\n",
    "    se = SearchOptions()           \n",
    "    query = 'movieId : %s' % the_id \n",
    "    se.commonparams.q(query) \n",
    "    res = conn[dt].search(se)\n",
    "    src = res.result.response.docs[0]\n",
    "    if 'factor' in src:\n",
    "        raw_vec = src['factor']\n",
    "        # our script actually uses the list form for the query vector and handles conversion internally\n",
    "        query_vec = reverse_convert(raw_vec[0])\n",
    "        qry = fn_query(query_vec, q=q, cosine=True)\n",
    "        se = SearchOptions()\n",
    "        se.commonparams.q(qry).fl(\"* score\").fq(q)\n",
    "        results = conn[dt].search(se)\n",
    "        hits = results.result.response.docs\n",
    "        return src, hits[1:num+1]\n",
    "    \n",
    "def get_user_recs(the_id, q=\"\", num=10, index=\"demo\"):\n",
    "    \"\"\"\n",
    "    Given a user id, execute the recommendation function score query to find top movies, ranked by predicted rating\n",
    "    \"\"\"\n",
    "    se = SearchOptions()           \n",
    "    query = 'id : %s' % the_id \n",
    "    se.commonparams.q(query)                           \n",
    "    res = conn[\"users\"].search(se)  \n",
    "    src = res.result.response.docs[0]\n",
    "    if 'factor' in src:\n",
    "        raw_vec = src['factor']\n",
    "        # our script actually uses the list form for the query vector and handles conversion internally\n",
    "        query_vec = reverse_convert(raw_vec)\n",
    "        qry = fn_query(query_vec, q=q, cosine=False)\n",
    "        se = SearchOptions()\n",
    "        se.commonparams.q(qry).fl(\"* score\").fq(q)      \n",
    "        results = conn[\"movies_vector\"].search(se)                        \n",
    "        hits = results.result.response.docs\n",
    "        return src, hits[:num]\n",
    "\n",
    "def get_movies_for_user(the_id, num=10, index=\"demo\"):\n",
    "    \"\"\"\n",
    "    Given a user id, get the movies rated by that user, from highest- to lowest-rated.\n",
    "    \"\"\"\n",
    "    se = SearchOptions()\n",
    "    se.commonparams.q(\"userId:%s\" % the_id).rows(num).sort(\"rating desc\")\n",
    "    res = conn[\"ratings\"].search(se) \n",
    "    hits = res.result.response.docs\n",
    "    ids = [h['movieId'] for h in hits]\n",
    "    \n",
    "    ids_disjunction_str = \" OR movieId:\".join(map(str, ids))\n",
    "    query = 'movieId : %s' % ids_disjunction_str\n",
    "    se = SearchOptions()\n",
    "    se.commonparams.q(query).fl('tmdbId, title')\n",
    "    res = conn[\"movies_vector\"].search(se)\n",
    "    tmdbids = res.result.response.docs\n",
    "    return tmdbids\n",
    "\n",
    "            \n",
    "def display_user_recs(the_id, q=\"*\", num=10, num_last=10, index=\"demo\"):\n",
    "    user, recs = get_user_recs(the_id, q, num, index)\n",
    "    user_movies = get_movies_for_user(the_id, num_last, index)\n",
    "    # check that posters can be displayed\n",
    "    first_movie = user_movies[0]\n",
    "    first_im_url = get_poster_url(int(first_movie['tmdbId'][0]))\n",
    "    if first_im_url == \"NA\":\n",
    "        display(HTML(\"<i>Cannot import tmdbsimple. No movie posters will be displayed!</i>\"))\n",
    "    if first_im_url == \"KEY_ERR\":\n",
    "        display(HTML(\"<i>Key error accessing TMDb API. Check your API key. No movie posters will be displayed!</i>\"))\n",
    "        \n",
    "    # display the movies that this user has rated highly\n",
    "    display(HTML(\"<h2>Get recommended movies for user id %s</h2>\" % the_id))\n",
    "    display(HTML(\"<h4>The user has rated the following movies highly:</h4>\"))\n",
    "    user_html = \"<table border=0>\"\n",
    "    i = 0\n",
    "    for movie in user_movies:\n",
    "        movie_im_url = get_poster_url(int(movie['tmdbId'][0]))\n",
    "        movie_title = movie['title']\n",
    "        user_html += \"<td><h5>%s</h5><img src=%s width=150></img></td>\" % (movie_title, movie_im_url)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            user_html += \"</tr><tr>\"\n",
    "    user_html += \"</tr></table>\"\n",
    "    display(HTML(user_html))\n",
    "    # now display the recommended movies for the user\n",
    "    display(HTML(\"<br>\"))\n",
    "    display(HTML(\"<h2>Recommended movies:</h2>\"))\n",
    "    rec_html = \"<table border=0>\"\n",
    "    i = 0\n",
    "    for rec in recs:\n",
    "        r_im_url = get_poster_url(int(rec['tmdbId'][0]))\n",
    "        r_score = rec['score']\n",
    "        r_title = rec['title']\n",
    "        rec_html += \"<td><h5>%s</h5><img src=%s width=150></img></td><td><h5>%2.3f</h5></td>\" % (r_title, r_im_url, r_score)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            rec_html += \"</tr><tr>\"\n",
    "    rec_html += \"</tr></table>\"\n",
    "    display(HTML(rec_html))\n",
    "\n",
    "    \n",
    "def display_similar(the_id, q=\"*\", num=10, index=\"demo\", dt=\"movies_vector\"):\n",
    "    \"\"\"\n",
    "    Display query movie, together with similar movies and similarity scores, in a table\n",
    "    \"\"\"\n",
    "    movie, recs = get_similar(the_id, q, num, index, dt)\n",
    "    q_im_url = get_poster_url(int(movie['tmdbId'][0]))\n",
    "    if q_im_url == \"NA\":\n",
    "        display(HTML(\"<i>Cannot import tmdbsimple. No movie posters will be displayed!</i>\"))\n",
    "    if q_im_url == \"KEY_ERR\":\n",
    "        display(HTML(\"<i>Key error accessing TMDb API. Check your API key. No movie posters will be displayed!</i>\"))\n",
    "        \n",
    "    display(HTML(\"<h2>Get similar movies for:</h2>\"))\n",
    "    display(HTML(\"<h4>%s</h4>\" % movie['title']))\n",
    "    if q_im_url != \"NA\":\n",
    "        display(Image(q_im_url, width=200))\n",
    "    display(HTML(\"<br>\"))\n",
    "    display(HTML(\"<h2>People who liked this movie also liked these:</h2>\"))\n",
    "    sim_html = \"<table border=0>\"\n",
    "    i = 0\n",
    "    for rec in recs:\n",
    "        r_im_url = get_poster_url(int(rec['tmdbId'][0]))\n",
    "        r_score = rec['score']\n",
    "        r_title = rec['title']\n",
    "        sim_html += \"<td><h5>%s</h5><img src=%s width=150></img></td><td><h5>%2.3f</h5></td>\" % (r_title, r_im_url, r_score)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            sim_html += \"</tr><tr>\"\n",
    "    sim_html += \"</tr></table>\"\n",
    "    display(HTML(sim_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, you're ready to generate some recommendations.\n",
    "\n",
    "### 5(a) Find similar movies for a given movie\n",
    "\n",
    "To start, you can find movies that are _similar_ to a given movie. This similarity score is computed from the model factor vectors for each movie. Recall that the ALS model you trained earlier is a collaborative filtering model, so the similarity between movie vectors will be based on the _rating co-occurrence_ of the movies. In other words, two movies that tend to be rated highly by a user will tend to be more similar. It is common to use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of the movie factor vectors as a measure of the similarity between two movies.\n",
    "\n",
    "Using this similarity you can show recommendations along the lines of _people who liked this movie also liked these_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_similar(2628, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So we see that people who like Star Wars tend like other sci-fi movies (including other Star Wars films), as well as some action and drama.\n",
    "\n",
    "> _Note:_ since we are using a very small dataset, results may not be as good as those for the same model trained on a larger dataset.\n",
    "\n",
    "Now you will see the power and flexibility that comes from using a search engine to generate recommendations. Solr allows you to tweak the results returned by the recommendation query using any standard search query or filter - from free text search through to filters based on time and geo-location (or any other piece of metadata you can think of).\n",
    "\n",
    "For example, perhaps you want to remove any movies with \"matrix\" in the title from the recommendations. You can do this by simply passing a valid Solr query string to the recommendation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_similar(2628, num=5, q='NOT(title:matrix)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or you may want to ensure that only valid children's movies are shown to young viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_similar(1, num=5, q=\"genres:children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Feel free to check out the documentation for the Solr [query string query](https://lucene.apache.org/solr/guide/6_6/searching.html) and play around with the various queries you can construct by passing in a query string as `q` in the recommendation function above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5(b) Find movies to recommend to a user\n",
    "\n",
    "Now, you're ready to generate some movie recommendations, personalized for a specific user.\n",
    "\n",
    "Given a user, you can recommend movies to that user based on the predicted ratings from your model. In a similar manner to the similar movie recommendations, this predicted rating score is computed from the model factor vector for the user and the factor vectors for each movie. Recall that the collaborative filtering model means that, at a high level, we will recommend movies _liked by other users who liked the same movies as the given user_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_user_recs(12, num=5, num_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Again, note that since we are using a very small dataset, the results may not be too good. However, we can see that this user seems to like some sci-fi, some horror and some comedy films. The recommended movies fall broadly into these categories and seem to be somewhat reasonable.\n",
    "\n",
    "Next, you can again apply the power of Solr's filtering capabilities to your recommendation engine. Let's say you only want to recommend more recent movies (say, from the past 5 years). This can be done by adding a date math query to the recommendation function query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_user_recs(12, num=5, num_last=5, q=\"release_date:[2012 TO *]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that the recommendation include only recent movies, and this time they seem to be heavily tilted to sci-fi and fantasy genres.\n",
    "\n",
    "As you did with the similar movies recommendations, feel free to play around with the various queries you could pass into the user recommendation query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
